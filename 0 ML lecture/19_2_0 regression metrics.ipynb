{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642781e7",
   "metadata": {},
   "source": [
    "# **Regression Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631864c",
   "metadata": {},
   "source": [
    "There multiple regression metrices.\n",
    "1. MAE (mean absolute error)\n",
    "    - MAE is the average of absolute differences between the actual and predicted values (on 'w' slope).\n",
    "    - Shows the average of erros in the predictions btwn actual and predicted values $y - \\hat{y}$.\n",
    "    - Pros :\n",
    "        - Simple to understand and interpret.\n",
    "        - Not affected by outliers.\n",
    "        - Gives output in same unit as the target data.\n",
    "        - Robust to noise in the data.\n",
    "    - Cons :\n",
    "        - Does not take into account the direction of the error.\n",
    "        - Sensitive to the scale of the data.\n",
    "        - Not a good measure of the quality of the model.\n",
    "    - **IF YOUR DATA HAS MORE OUTLIERS, THEN USE MAE**\n",
    "2. MSE (mean squared error)\n",
    "    - MSE is the average of squared differences between the actual and predicted values.\n",
    "    - Pros :\n",
    "        - Can use it for a loss function\n",
    "            - It is the function that is minimized during the training process to optimize the model's parameters\n",
    "        - Differentiable, making it ideal for optimization algorithms like Gradient Descent.\n",
    "        - Penalizes larger errors more heavily than smaller ones.\n",
    "    - Cons :\n",
    "        - Highly sensitive to outliers.\n",
    "        - Not robust to outliers.\n",
    "        - Resulting error is in squared units, making it less intuitive to interpret.\n",
    "    - **IF YOUR DATA HAS LESS OUTLIERS, THEN USE MSE**\n",
    "3. RMSE (root of mean squared error)\n",
    "    - RMSE is the square root of the average of squared differences between the actual and predicted values.\n",
    "    - Pros :\n",
    "        - Resulting error is in the same unit as the target variable, making it easy to interpret.\n",
    "        - Penalizes larger errors more significantly than smaller ones.\n",
    "        - Differentiable, which is useful for optimization.\n",
    "        - Gives output in same unit as the target data.\n",
    "    - Cons :\n",
    "        - Sensitive to outliers.  \n",
    "        - Not robust to outliers. \n",
    "4. R2 score\n",
    "    - R2 score is the ratio of the sum of squares of the differences between the actual and predicted values to the sum of squares of the differences between the actual and the mean of the actual values.\n",
    "    - Only used for linear regression models.\n",
    "    - Formula: $R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$\n",
    "    - Pros :\n",
    "        - Scale-independent, allowing comparison across different models and datasets.\n",
    "        - Provides an intuitive measure of the proportion of variance explained by the model.\n",
    "        - Standardized metric, typically ranging from 0 to 1.\n",
    "    - Cons :\n",
    "        - Always increases, or decreases, or stays the same as more features are added, regardless of their relevance.\n",
    "        - Does not account for the complexity of the model (overfitting).\n",
    "        - Sensitive to outliers.\n",
    "5. Adjusted R2 score\n",
    "    - Adjusted R2 score is the ratio of the sum of squares of the differences between the actual and predicted values to the sum of squares of the differences between the actual and the mean of the actual values, adjusted for the number of features in the model.\n",
    "    - Formula: $R^2_{adj} = 1 - \\frac{(1 - R^2) \\times (n - 1)}{n - k - 1}$\n",
    "        - $R^2$ = R2 score\n",
    "        - $n$ = number of data points\n",
    "        - $k$ = number of features\n",
    "    - Pros :\n",
    "        - Penalizes the addition of irrelevant features.\n",
    "        - More reliable for comparing models with different numbers of predictors.\n",
    "        - Prevents overestimation of the model's explanatory power.\n",
    "    - Cons :\n",
    "        - Can be negative if the model is extremely poor.\n",
    "        - Not as intuitive as the standard R2 score.\n",
    "        - Still sensitive to outliers.\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
