{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf719fa8",
   "metadata": {},
   "source": [
    "# **Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82236eeb",
   "metadata": {},
   "source": [
    "3 types of gradient descent:\n",
    "\n",
    "1. Batch gradient descent\n",
    "    - Use the entire dataset to calculate the gradient\n",
    "    - More stable but slower\n",
    "2. Stochastic gradient descent\n",
    "    - Use one sample at a time to calculate the gradient\n",
    "    - Faster but less stable\n",
    "3. Mini-batch gradient descent\n",
    "    - Use a small subset of the dataset to calculate the gradient\n",
    "    - Faster than batch but more stable than stochastic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711b6a9",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent is an optimization algorithm used to minimize the cost function by calculating the gradient of the cost function with respect to the parameters for the entire training dataset.\n",
    "\n",
    "\n",
    "The cost function $J(\\theta)$ is defined as:\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "The gradient of the cost function with respect to each parameter $\\theta_j$ is:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "In vector notation:\n",
    "$$\\nabla_\\theta J(\\theta) = \\frac{1}{m} X^T (X\\theta - y)$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $m$ is the number of training examples.\n",
    "- $X$ is the feature matrix.\n",
    "- $y$ is the target vector.\n",
    "- $h_\\theta(x)$ is the hypothesis function, which represents the predicted value for a given input $x$.\n",
    "\n",
    "theta represents the vector of parameters (weights) that the model learns.\n",
    "It defines the hypothesis function: $h_\\theta(x) = \\theta_0 + \\theta_1*x_1 + ... + \\theta_n*x_n$\n",
    "In vector notation, the prediction is calculated as: $h_\\theta(X) = X @ \\theta$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0393dbe",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# **Derivation of the Matrix Gradient for Linear Regression**\n",
    "\n",
    "To derive the formula $\\frac{2}{n} X^T (\\hat{y} - y)$, we start with the definition of the Cost Function in matrix form and apply the rules of Matrix Calculus.\n",
    "\n",
    "**1. Defining the Variables in Matrix Form**\n",
    "\n",
    "In multiple linear regression, we represent our data as matrices:\n",
    "\n",
    "- $X$: The input matrix of size $(n \\times k)$, where $n$ is the number of samples and $k$ is the number of features.\n",
    "- $w$: The weight vector of size $(k \\times 1)$.\n",
    "- $y$: The actual target vector of size $(n \\times 1)$.\n",
    "- $\\hat{y}$: The prediction vector, calculated as $\\hat{y} = Xw$.\n",
    "\n",
    "**The Cost Function (MSE)**\n",
    "\n",
    "The Mean Squared Error is the average of the squared differences. In matrix notation, the \"squared sum\" is the squared norm of the error vector:\n",
    "\n",
    "$$J(w) = \\frac{1}{n} \\|Xw - y\\|^2$$\n",
    "\n",
    "Using the property $\\|a\\|^2 = a^T a$, we can expand this:\n",
    "\n",
    "\n",
    "$$J(w) = \\frac{1}{n} (Xw - y)^T (Xw - y)$$\n",
    "\n",
    "\n",
    "Using the distributive property of transpose $(A - B)^T = A^T - B^T$:\n",
    "\n",
    "\n",
    "$$J(w) = \\frac{1}{n} ( (Xw)^T - y^T ) (Xw - y)$$\n",
    "\n",
    "$$J(w) = \\frac{1}{n} ( (w^T X^T - y^T) (Xw - y) )$$\n",
    "\n",
    "Now, we multiply the terms:\n",
    "\n",
    "\n",
    "$$J(w) = \\frac{1}{n} ( w^T X^T X w - w^T X^T y - y^T X w + y^T y )$$\n",
    "\n",
    "Note: Since $w^T X^T y$ is a scalar (a single number), it is equal to its own transpose $(w^T X^T y)^T = y^T X w$. We can combine the middle two terms:\n",
    "\n",
    "$$J(w) = \\frac{1}{n} ( w^T X^T X w - 2w^T X^T y + y^T y )$$\n",
    "\n",
    "\n",
    "We now take the partial derivative with respect to the vector $w$, denoted as $\\nabla_w J(w)$. We use three standard rules of Matrix Calculus:\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} (w^T A w) = 2Aw$ (for symmetric $A$)\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} (w^T A) = A$\n",
    "\n",
    "The derivative of a constant ($y^T y$) is 0.\n",
    "\n",
    "Applying these to our terms:\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} (w^T (X^T X) w) = 2 X^T X w$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} (-2 w^T X^T y) = -2 X^T y$\n",
    "\n",
    "Combine them back:\n",
    "\n",
    "\n",
    "$$\\nabla_w J(w) = \\frac{1}{n} ( 2 X^T X w - 2 X^T y )$$\n",
    "\n",
    "\n",
    "Factor out the $\\frac{2}{n}$ and the $X^T$:\n",
    "\n",
    "\n",
    "$$\\nabla_w J(w) = \\frac{2}{n} X^T (X w - y)$$\n",
    "\n",
    "Since $Xw = \\hat{y}$ (our prediction), we get the formula from your image:\n",
    "\n",
    "\n",
    "$$\\nabla_w J(w) = \\frac{2}{n} X^T (\\hat{y} - y)$$\n",
    "\n",
    "Why use this version?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a42f4e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# Batch Gradient Descent for Multiple Features\n",
    "In multiple linear regression, we have multiple input features ($X$) and we want to predict a target ($y$). The model learns a coefficient vector ($m$) corresponding to the features and a single intercept ($b$).\n",
    "\n",
    "## 1. The Hypothesis (Equation)\n",
    "The prediction $\\hat{y}$ is calculated as the dot product of inputs and coefficients plus the intercept:\n",
    "$$ \\hat{y} = m_1x_1 + m_2x_2 + \\dots + m_nx_n + b $$\n",
    "In vectorized form (using matrix multiplication), where $X$ is our input matrix of shape `(samples, features)` and $m$ is our coefficient vector of shape `(features, 1)`:\n",
    "$$ \\hat{y} = X \\cdot m + b $$\n",
    "\n",
    "## 2. The Cost Function\n",
    "We use the **Mean Squared Error (MSE)** to measure how far off our predictions are from the actual values:\n",
    "- $i$ refers to the $i$-th training example (or data point/row)\n",
    "$$ J(m, b) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "\n",
    "## 3. The Gradient Formulas\n",
    "To minimize the cost, we calculate the derivative (slope) of the cost function with respect to $m$ and $b$.\n",
    "\n",
    "**Gradient for Coefficients ($\\frac{\\partial J}{\\partial m}$):**\n",
    "$$ \\frac{\\partial J}{\\partial m} = \\frac{2}{n} X^T \\cdot (\\hat{y} - y) $$\n",
    "**Gradient for Intercept ($\\frac{\\partial J}{\\partial b}$):**\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum (\\hat{y} - y) $$\n",
    "\n",
    "## 4. The Update Rule\n",
    "We update the parameters by moving them in the opposite direction of the gradient, scaled by a `learning_rate` ($\\alpha$):\n",
    "$$ m_{new} = m_{old} - \\alpha \\times \\frac{\\partial J}{\\partial m} $$\n",
    "$$ b_{new} = b_{old} - \\alpha \\times \\frac{\\partial J}{\\partial b} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc619c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "### Derivation of the Gradient Formula\n",
    "To understand how we get $\\frac{\\partial J}{\\partial m} = \\frac{2}{n} X^T \\cdot (\\hat{y} - y)$, let's break it down using the Chain Rule.\n",
    "#### 1. The Scalar Derivative (Single Weight)\n",
    "First, we find the derivative for **one specific weight** ($m_j$) which corresponds to the $j$-th feature.\n",
    "**The Cost Function:**\n",
    "- Capital $J$ (The Numerator):\n",
    "    - What it is: This stands for the Cost Function (or Loss Function).\n",
    "    - Meaning: It represents the Total Error of your model across the entire dataset. Our goal in machine learning is always to minimize this big $J$.\n",
    "\n",
    "- Lowercase $j$ (The Subscript in $m_j$ ):\n",
    "    - What it is: This is an Index that points to a specific feature (column) in your data.\n",
    "    - Meaning: If you have 3 features (columns) in your dataset, $j$ loops from 1 to 3.\n",
    "    - If $j=1$: You are calculating the slope for the 1st weight ($m_1$) corresponding to the 1st feature (e.g., House Size).\n",
    "    - If $j=2$: You are calculating the slope for the 2nd weight ($m_2$) corresponding to the 2nd feature (e.g., Number of Rooms).\n",
    "\n",
    "- In machine learning notation, when you see a superscript in parentheses like $(i)$, it represents an Index (or a label), not math.\n",
    "- $ \\hat{y}^{(i)} $\n",
    "- $\\hat{y}$ (y-hat): This means the Predicted Value (what the model thinks the answer is).\n",
    "- $(i)$: This refers to the $i$-th row (or student, or house) in your dataset.\n",
    "- It is read as: \"The prediction for the $i$-th training example.\"\n",
    "\n",
    "$$ J(m, b) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "Using the **Chain Rule**, we take the derivative with respect to $m_j$:\n",
    "$$\\frac{\\partial J}{\\partial m_j} = \\frac{1}{n} \\sum_{i=1}^{n} 2(\\hat{y}^{(i)} - y^{(i)}) \\cdot \\frac{\\partial}{\\partial m_j}(\\hat{y}^{(i)} - y^{(i)})$$\n",
    "$$\\frac{\\partial}{\\partial m_j} (m_1x_{i1} + m_2x_{i2} + \\dots + m_jx_{ij} + \\dots + b - y^{(i)})$$\n",
    "\n",
    "- The derivative of $m_jx_{ij}$ with respect to $m_j$ is $x_{ij}$.\n",
    "- The derivative of all other terms ($m_1, b, y^{(i)}$) is $0$ because they are constants relative to $m_j$.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial m_j} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) \\cdot x_{ij}$$\n",
    "Since the prediction $\\hat{y}^{(i)}$ is a linear combination ($m_1x_1 + \\dots + m_j x_j + \\dots$), the derivative with respect to $m_j$ is simply the input feature value $x_j^{(i)}$.\n",
    "Substituting this back in, we get the gradient for one weight:\n",
    "$$ \\frac{\\partial J}{\\partial m_j} = \\frac{2}{n} \\sum_{i=1}^{n} \\underbrace{(\\hat{y}^{(i)} - y^{(i)})}_{\\text{Error}} \\cdot \\underbrace{x_j^{(i)}}_{\\text{Input}} $$\n",
    "$$or$$\n",
    "$$m_{j(new)} = m_{j(old)} - \\alpha \\left[ \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})x_{ij} \\right]$$\n",
    "**Intuition:** The gradient for a weight is the average of the **Error** multiplied by the **Input Feature**.\n",
    "---\n",
    "#### 2. Vectorizing (Matrix Form)\n",
    "To calculate this for **all weights simultaneously**, we utilize matrix algebra.\n",
    "*   In the scalar sum $\\sum (\\text{Error} \\times x_j)$, we are dotting the Error vector with the column of feature $j$.\n",
    "*   To do this for *all* features at once, we use the full input matrix $X$.\n",
    "**Dimensions Check:**\n",
    "*   $X$ has shape $(n, \\text{features})$.\n",
    "*   $(\\hat{y} - y)$ has shape $(n, 1)$.\n",
    "To get a result of shape $(\\text{features}, 1)$ (one gradient per weight), we must transpose $X$:\n",
    "$$ \\underbrace{X^T}_{(\\text{features}, n)} \\cdot \\underbrace{(\\hat{y} - y)}_{(n, 1)} = \\text{Result with shape } (\\text{features}, 1) $$\n",
    "This matrix multiplication $X^T \\cdot (\\hat{y} - y)$ performs the summation $\\sum (\\text{Error} \\times \\text{Input})$ for every feature in parallel.\n",
    "#### 3. Final Formula\n",
    "Re-attaching the constant $\\frac{2}{n}$, we arrive at the final vectorized equation:\n",
    "$$ \\frac{\\partial J}{\\partial m} = \\frac{2}{n} X^T \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc6d63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd80ad95",
   "metadata": {},
   "source": [
    "### Steps to Derive the Gradient Formula\n",
    "The transition from the **Cost Function** ($J$) to the **Gradient** ($\\frac{\\partial J}{\\partial m_j}$) involves taking the derivative using the **Chain Rule**.\n",
    "#### 1. Start with the Cost Function\n",
    "We begin with the Mean Squared Error (MSE), which measures the average squared difference between predictions and actual values.\n",
    "$$ J = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "#### 2. Apply the Derivative\n",
    "We want to find how $J$ changes with respect to a weight $m_j$. So, we take the partial derivative:\n",
    "$$ \\frac{\\partial J}{\\partial m_j} = \\frac{\\partial}{\\partial m_j} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)})^2 \\right) $$\n",
    "Since the derivative of a sum is the sum of derivatives, we can move the derivative inside the summation:\n",
    "$$ = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial m_j} (\\underbrace{\\hat{y}^{(i)} - y^{(i)}}_{u})^2 $$\n",
    "#### 3. Use the Power Rule (Outer Function)\n",
    "Let $u = (\\hat{y}^{(i)} - y^{(i)})$. The expression is essentially $u^2$.\n",
    "The derivative of $u^2$ with respect to $u$ is $2u$.\n",
    "$$ \\frac{d}{du}(u^2) = 2u $$\n",
    "So, we bring down the **2**:\n",
    "$$ = \\frac{1}{n} \\sum_{i=1}^{n} 2(\\hat{y}^{(i)} - y^{(i)}) \\times \\dots $$\n",
    "This explains why $\\frac{1}{n}$ becomes $\\frac{2}{n}$.\n",
    "#### 4. Use the Chain Rule (Inner Function)\n",
    "Now, because $u$ is a function of $m_j$, we must multiply by the derivative of the inside term $u$ with respect to $m_j$.\n",
    "$$ \\text{Inner Derivative} = \\frac{\\partial}{\\partial m_j} (\\hat{y}^{(i)} - y^{(i)}) $$\n",
    "Since $y^{(i)}$ is a constant (actual data), its derivative is 0. We are left with the derivative of $\\hat{y}^{(i)}$.\n",
    "#### 5. Combine Results\n",
    "Putting it all together gives us the formula in the image:\n",
    "$$ \\frac{\\partial J}{\\partial m_j} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)}) \\times \\frac{\\partial}{\\partial m_j}(\\hat{y}^{(i)}) $$\n",
    "> **In English:** The gradient is the average of (2 * Error * Slope of Prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd858740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9affec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd2caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
