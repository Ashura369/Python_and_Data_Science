{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefaf0cd",
   "metadata": {},
   "source": [
    "# GridSearchCV Parameters Explained\n",
    "\n",
    "When you initialize `GridSearchCV`, you can tweak several parameters to control how the search behaves.\n",
    "\n",
    "```python\n",
    "GridSearchCV(estimator, param_grid, scoring=None, n_jobs=None, cv=None, verbose=0, refit=True)\n",
    "```\n",
    "\n",
    "Here is a breakdown of the most important parameters:\n",
    "\n",
    "### 1. `estimator`\n",
    "*   **What it is:** The machine learning model you want to tune.\n",
    "*   **Use Case:** Pass your model object here (e.g., `DecisionTreeClassifier()`, `LogisticRegression()`, or a `Pipeline`).\n",
    "*   **Note:** You should usually pass an *unfitted* model instance.\n",
    "\n",
    "### 2. `param_grid`\n",
    "*   **What it is:** A dictionary (or list of dictionaries) defining the parameter names and the values to try.\n",
    "*   **Use Case:** This is the core of the search.\n",
    "    *   Example: `{'max_depth': [10, 20], 'min_samples_leaf': [1, 5]}`\n",
    "*   **Tip:** Be careful not to add too many options, or the search will take forever!\n",
    "\n",
    "### 3. `scoring`\n",
    "*   **What it is:** The metric used to evaluate which model is \"best\".\n",
    "*   **Use Case:**\n",
    "    *   For Classification: `'accuracy'`, `'precision'`, `'recall'`, `'f1'`, `'roc_auc'`.\n",
    "    *   For Regression: `'neg_mean_squared_error'`, `'r2'`.\n",
    "*   **Default:** If you don't set this, it uses the model's default `.score()` method (usually accuracy for classifiers).\n",
    "\n",
    "### 4. `cv` (Cross-Validation)\n",
    "*   **What it is:** Determines how the data is split for validation.\n",
    "*   **Use Case:**\n",
    "    *   **Integer (e.g., `cv=5`):** The standard way. Uses K-Fold cross-validation (5 folds is a good default).\n",
    "    *   **Cross-Validation Splitter:** You can pass a specific splitter object (like `StratifiedKFold`) if you need advanced control (e.g., for imbalanced data).\n",
    "\n",
    "### 5. `n_jobs`\n",
    "*   **What it is:** Number of CPU cores to use for parallel processing.\n",
    "*   **Use Case:**\n",
    "    *   `n_jobs=1`: Run sequentially (slowest).\n",
    "    *   `n_jobs=-1`: Use **all available cores** (fastest). Highly recommended for large grids!\n",
    "\n",
    "### 6. `verbose`\n",
    "*   **What it is:** Controls how much information is printed while the search is running.\n",
    "*   **Use Case:**\n",
    "    *   `verbose=0`: Silent (no output).\n",
    "    *   `verbose=1` or `2`: Prints progress updates (e.g., \"Fitting 5 folds for each of 20 candidates\"). Useful to know if your code is stuck or just working hard.\n",
    "\n",
    "### 7. `refit`\n",
    "*   **What it is:** Whether to re-train the best model on the *entire* dataset after the search is done.\n",
    "*   **Use Case:**\n",
    "    *   `refit=True` (Default): Highly recommended. It allows you to use `grid_search.predict()` immediately after fitting without needing to manually retrain the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ddbc5",
   "metadata": {},
   "source": [
    "# Components of `param_grid` (Decision Tree Hyperparameters)\n",
    "When tuning a Decision Tree, these are the most critical settings you will put inside your `param_grid`.\n",
    "### 1. `criterion`\n",
    "*   **What it is:** The function used to measure the quality of a split. It decides *how* the tree chooses the best question to ask at each node.\n",
    "*   **Code Example:**\n",
    "    ```python\n",
    "    'criterion': ['gini', 'entropy']\n",
    "    ```\n",
    "*   **`\"gini\"` (Gini Impurity):**\n",
    "    *   **Meaning:** Measures how often a randomly chosen element would be incorrectly labeled.\n",
    "    *   **Pros:** Computationally faster because it doesn't use logarithms.\n",
    "    *   **Cons:** Tends to isolate the most frequent class in its own branch.\n",
    "*   **`\"entropy\"` (Information Gain):**\n",
    "    *   **Meaning:** Measures the disorder or uncertainty in the data.\n",
    "    *   **Pros:** Can produce slightly more balanced trees.\n",
    "    *   **Cons:** Slower to calculate due to logarithmic operations.\n",
    "    ## How GridSearchCV Decides what to use: Gini or Entropy?\n",
    "    The code runs a contest:\n",
    "    ### 1. Round 1: Testing `criterion='gini'`\n",
    "    *   It builds a tree using Gini Impurity logic.\n",
    "    *   It tests on validation data.\n",
    "    *   It calculates accuracy (e.g., **90%**).\n",
    "    ### 2. Round 2: Testing `criterion='entropy'`\n",
    "    *   It builds a tree using Information Gain logic.\n",
    "    *   It tests on validation data.\n",
    "    *   It calculates accuracy (e.g., **91%**).\n",
    "    ### The Final Decision\n",
    "    | Parameter | Validation Accuracy |\n",
    "    | :--- | :--- |\n",
    "    | `criterion='gini'` | 90% |\n",
    "    | **`criterion='entropy'`** | **91% (Winner!)** |\n",
    "    It chooses **Entropy** because it performed slightly better on this specific dataset.\n",
    "---\n",
    "### 2. `max_depth`\n",
    "*   **What it is:** The maximum height the tree is allowed to grow.\n",
    "*   **Code Example:**\n",
    "    ```python\n",
    "    'max_depth': [None, 10, 20, 30]\n",
    "    ```\n",
    "*   **`None` (The Default):**\n",
    "    *   **Meaning:** \"No Limit.\" The tree will keep splitting and growing until every single leaf is \"pure\" (contains only one type of class) or until it runs out of data (fewer than `min_samples_split`).\n",
    "    *   **Pros:** Can learn extremely complex and detailed patterns.\n",
    "    *   **Cons:** Very high risk of **Overfitting**. It might memorize the training data perfectly (including noise) but fail on new data.\n",
    "*   **`10` (Shallow Depth):**\n",
    "    *   **Meaning:** The tree stops growing after 10 levels.\n",
    "    *   **Pros:** Creates a simpler, more general model. Good for preventing overfitting.\n",
    "    *   **Cons:** Risk of **Underfitting**. It might be *too* simple to capture the real patterns in the data.\n",
    "*   **`20` & `30` (Medium to High Depth):**\n",
    "    *   **Meaning:** The tree can grow up to 20 or 30 levels.\n",
    "    *   **Use Case:** These are \"middle ground\" options.\n",
    "    *   By testing `10`, `20`, and `30`, you are asking GridSearchCV: *\"Is a simple tree (10) better? Or do we need a moderately complex tree (20)? Or a very complex tree (30)?\"*\n",
    "    ## How GridSearchCV Decides what to use: None, 10, 20, or 30?\n",
    "    The code runs a contest:\n",
    "    ### 1. Round 1: Testing `max_depth=None`\n",
    "    *   It builds a tree with **NO limit**.\n",
    "    *   It tests this tree on the **validation data**.\n",
    "    *   It calculates the average accuracy (e.g., **85%**).\n",
    "    ### 2. Round 2: Testing `max_depth=10`\n",
    "    *   It builds a new tree that stops at **depth 10**.\n",
    "    *   It tests this tree on the validation data.\n",
    "    *   It calculates the average accuracy (e.g., **92%**).\n",
    "    *   *Result: This is currently the best.*\n",
    "    ### 3. Round 3: Testing `max_depth=20`\n",
    "    *   It builds a tree that stops at **depth 20**.\n",
    "    *   It tests it on the validation data.\n",
    "    *   It calculates the average accuracy (e.g., **89%**).\n",
    "    *   *Result: Worse than depth 10.*\n",
    "    ### 4. Round 4: Testing `max_depth=30`\n",
    "    *   It builds a tree that stops at **depth 30**.\n",
    "    *   It tests it on the validation data.\n",
    "    *   It calculates the average accuracy (e.g., **87%**).\n",
    "    ### The Final Decision\n",
    "    | Parameter | Validation Accuracy |\n",
    "    | :--- | :--- |\n",
    "    | `max_depth=None` | 85% |\n",
    "    | **`max_depth=10`** | **92% (Highest Accuracy !!!)** |\n",
    "    | `max_depth=20` | 89% |\n",
    "    | `max_depth=30` | 87% |\n",
    "    It declares `max_depth=10` as the winner because it achieved the **highest average accuracy** on the unseen validation data.\n",
    "---\n",
    "### 3. `min_samples_split`\n",
    "*   **What it is:** The minimum number of samples required to split an internal node.\n",
    "*   **Code Example:**\n",
    "    ```python\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "    ```\n",
    "*   **`2` (Low Limit):**\n",
    "    *   **Meaning:** Even if a node has only 2 samples, the tree is allowed to split it further.\n",
    "    *   **Pros:** Captures very fine details.\n",
    "    *   **Cons:** High risk of **Overfitting**. It might create a rule just for 2 specific people in the dataset.\n",
    "*   **`10` (High Limit):**\n",
    "    *   **Meaning:** A node must have at least 10 samples to be considered for a split. If it has 9, it stops growing there.\n",
    "    *   **Pros:** Forces the tree to learn broader rules that apply to groups of at least 10. Good for **Regularization**.\n",
    "    *   **Cons:** Might miss finer details (Underfitting).\n",
    "    ## How GridSearchCV Decides what to use: 2, 5, or 10?\n",
    "    The code runs a contest:\n",
    "    ### 1. Round 1: Testing `min_samples_split=2`\n",
    "    *   Builds a very detailed tree.\n",
    "    *   Validation Accuracy: **88%** (Maybe it overfitted).\n",
    "    ### 2. Round 2: Testing `min_samples_split=5`\n",
    "    *   Builds a slightly more constrained tree.\n",
    "    *   Validation Accuracy: **91%**.\n",
    "    ### 3. Round 3: Testing `min_samples_split=10`\n",
    "    *   Builds a very general tree.\n",
    "    *   Validation Accuracy: **89%** (Too general).\n",
    "    ### The Final Decision\n",
    "    | Parameter | Validation Accuracy |\n",
    "    | :--- | :--- |\n",
    "    | `min_samples_split=2` | 88% |\n",
    "    | **`min_samples_split=5`** | **91% (Winner!)** |\n",
    "    | `min_samples_split=10` | 89% |\n",
    "    It chooses **5** as the best balance.\n",
    "---\n",
    "### 4. `min_samples_leaf`\n",
    "*   **What it is:** The minimum number of samples required to be at a leaf node (the end of a branch).\n",
    "*   **Code Example:**\n",
    "    ```python\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "    ```\n",
    "*   **`1` (Default):**\n",
    "    *   **Meaning:** A leaf can end with just 1 sample.\n",
    "    *   **Pros:** Can perfectly classify every single training point.\n",
    "    *   **Cons:** Very sensitive to **Noise**. One outlier data point can create its own leaf.\n",
    "*   **`4` (Higher Value):**\n",
    "    *   **Meaning:** Every leaf must represent at least 4 samples.\n",
    "    *   **Pros:** Smooths the model. It ignores \"freak accidents\" or outliers that don't have at least 3 other similar friends.\n",
    "    *   **Cons:** Might lose precision on small but valid groups.\n",
    "    ## How GridSearchCV Decides what to use: 1, 2, or 4?\n",
    "    The code runs a contest:\n",
    "    ### 1. Round 1: Testing `min_samples_leaf=1`\n",
    "    *   Builds a tree that fits every point.\n",
    "    *   Validation Accuracy: **86%** (It memorized noise).\n",
    "    ### 2. Round 2: Testing `min_samples_leaf=2`\n",
    "    *   Validation Accuracy: **89%**.\n",
    "    ### 3. Round 3: Testing `min_samples_leaf=4`\n",
    "    *   Validation Accuracy: **90%**.\n",
    "    ### The Final Decision\n",
    "    | Parameter | Validation Accuracy |\n",
    "    | :--- | :--- |\n",
    "    | `min_samples_leaf=1` | 86% |\n",
    "    | `min_samples_leaf=2` | 89% |\n",
    "    | **`min_samples_leaf=4`** | **90% (Winner!)** |\n",
    "    It chooses **4** because smoothing out the noise helped the model generalize better.\n",
    "---\n",
    "### 5. `max_features`\n",
    "*   **What it is:** The number of features to consider when looking for the best split.\n",
    "*   **Code Example:**\n",
    "    ```python\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "    ```\n",
    "*   **`None` (Use All Features):**\n",
    "    *   **Meaning:** Look at every single column in your dataset to find the best split.\n",
    "    *   **Pros:** Finds the absolute best split possible at that moment.\n",
    "    *   **Cons:** Can be slow. Also, if one feature is super powerful, every tree will look the same (less diversity).\n",
    "*   **`\"sqrt\"` (Square Root):**\n",
    "    *   **Meaning:** If you have 100 features, only look at a random 10 ($\\sqrt{100}$) of them at each split.\n",
    "    *   **Pros:** Adds randomness! This makes the model more robust and less likely to overfit to one dominant feature. (This is the secret sauce of Random Forests).\n",
    "    *   **Cons:** Might miss the \"perfect\" split if the best feature wasn't in the random group.\n",
    "    ## How GridSearchCV Decides what to use: None or sqrt?\n",
    "    The code runs a contest:\n",
    "    ### 1. Round 1: Testing `max_features=None`\n",
    "    *   Builds a standard tree.\n",
    "    *   Validation Accuracy: **88%**.\n",
    "    ### 2. Round 2: Testing `max_features='sqrt'`\n",
    "    *   Builds a tree using random feature subsets.\n",
    "    *   Validation Accuracy: **89%**.\n",
    "    ### The Final Decision\n",
    "    | Parameter | Validation Accuracy |\n",
    "    | :--- | :--- |\n",
    "    | `max_features=None` | 88% |\n",
    "    | **`max_features='sqrt'`** | **89% (Winner!)** |\n",
    "    It chooses **sqrt** because the added randomness helped prevent overfitting.\n",
    "---\n",
    "### Full `param_grid` Example\n",
    "```python\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt']\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
