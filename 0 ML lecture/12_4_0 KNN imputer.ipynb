{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c56218e",
   "metadata": {},
   "source": [
    "# **KNN Imputer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36f42e",
   "metadata": {},
   "source": [
    "### Univariate vs. Multivariate Imputation\n",
    "\n",
    "**Univariate Imputation**\n",
    "Univariate imputation involves imputing missing values in a single feature (column) using only the available information from that same feature. Common methods include:\n",
    "- **Mean imputation**: Replacing missing values with the mean of the observed values in the column.\n",
    "- **Median imputation**: Replacing missing values with the median of the observed values in the column.\n",
    "- **Mode imputation**: Replacing missing values with the mode (most frequent value) of the observed values in the column.\n",
    "- **Constant imputation**: Replacing missing values with a specified constant value.\n",
    "\n",
    "*Pros/Cons*: Simple to implement but can lead to biased estimates and may underestimate the variance of the imputed variable, as it ignores relationships with other variables.\n",
    "\n",
    "**Multivariate Imputation**\n",
    "Multivariate imputation involves estimating missing values in a feature by taking into account the relationships with other features in the dataset. These methods leverage the correlation structure between variables to provide more accurate imputations. Common methods include:\n",
    "- **K-Nearest Neighbors (KNN) imputation**: Missing values are imputed based on the values of the k-nearest neighbors in the feature space. The \"distance\" to find neighbors considers all available features.\n",
    "- **Multiple Imputation by Chained Equations (MICE)**: An iterative process where each incomplete feature is imputed using a prediction model (e.g., linear regression) based on other features.\n",
    "- **PCA based imputation**: Using principal components to estimate missing values.\n",
    "\n",
    "*Pros/Cons*: Generally more sophisticated and can provide more accurate imputations, preserving relationships between variables better than univariate methods. However, they are more computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01108e",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, non-parametric, lazy learning algorithm primarily used for classification and regression tasks. It's considered \"lazy\" because it does not construct a model during the training phase; instead, it memorizes the entire training dataset. All computations are deferred until a prediction is requested.\n",
    "\n",
    "## How KNN Works\n",
    "\n",
    "The core principle of KNN is that data points that are similar tend to exist in close proximity within the feature space. When a new, unseen data point requires classification or a value prediction, KNN identifies its 'k' nearest neighbors from the training dataset.\n",
    "\n",
    "### For Classification Tasks:\n",
    "\n",
    "1.  **Choose the Value of K:** Select the number of nearest neighbors (`K`) to consider. `K` is typically a small, odd integer (e.g., 3, 5, 7) to prevent ties when determining the majority class.\n",
    "2.  **Calculate Distances:** For the new data point, compute its distance to *every* data point in the training dataset. Common distance metrics include:\n",
    "    *   **Euclidean Distance:** The straight-line distance between two points in Euclidean space. For two points $P_1 = (x_1, y_1)$ and $P_2 = (x_2, y_2)$, it's $\\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$. For $n$-dimensional space, it's $\\sqrt{\\sum_{i=1}^{n}(p_{1i} - p_{2i})^2}$. It's the most widely used metric.\n",
    "    *   **Manhattan Distance (L1 Norm):** The sum of the absolute differences of their Cartesian coordinates. For two points, it's $|x_2-x_1| + |y_2-y_1|$. For $n$-dimensional space, it's $\\sum_{i=1}^{n}|p_{1i} - p_{2i}|$. It represents distance travelled along axes at right angles.\n",
    "    *   **Minkowski Distance:** A generalization of Euclidean and Manhattan distances. It is defined as $(\\sum_{i=1}^{n}(|p_{1i} - p_{2i}|)^p)^{1/p}$. When $p=1$, it's Manhattan distance; when $p=2$, it's Euclidean distance.\n",
    "3.  **Identify K-Nearest Neighbors:** Sort the calculated distances in ascending order and select the `K` data points with the smallest distances. These are the K-nearest neighbors.\n",
    "4.  **Vote for Class Label:** Examine the class labels of these `K` neighbors. The new data point is assigned the class label that is most frequent among its `K` nearest neighbors (majority vote). In cases of ties with an even `K`, various strategies can be employed, such as taking the next nearest neighbor or assigning randomly.\n",
    "\n",
    "### For Regression Tasks:\n",
    "\n",
    "1.  **Choose the Value of K:** Similar to classification, select an appropriate `K`.\n",
    "2.  **Calculate Distances:** Compute the distance from the new data point to all training data points using a chosen distance metric (e.g., Euclidean).\n",
    "3.  **Identify K-Nearest Neighbors:** Select the `K` training data points closest to the new point.\n",
    "4.  **Calculate Average/Weighted Average:** The predicted value for the new data point is typically the average (mean) of the target values of its `K` nearest neighbors. A weighted average can also be used, where closer neighbors contribute more to the average.\n",
    "\n",
    "## Key Considerations and Characteristics:\n",
    "\n",
    "*   **Non-parametric:** KNN makes no assumptions about the underlying data distribution.\n",
    "*   **Lazy Learning:** No explicit training phase. All calculations occur during prediction, making it computationally expensive for large datasets during inference.\n",
    "*   **Feature Scaling:** KNN is sensitive to the scale of features because distance calculations are heavily influenced by features with larger ranges. It's crucial to scale (e.g., standardization or normalization) the features before applying KNN.\n",
    "*   **Choice of K:**\n",
    "    *   **Small K:** Can be noisy and sensitive to outliers, potentially leading to overfitting.\n",
    "    *   **Large K:** Smoothes out predictions, but may blur boundaries between classes or miss fine-grained patterns, potentially leading to underfitting.\n",
    "    *   The optimal `K` is often found through cross-validation.\n",
    "*   **Distance Metric:** The choice of distance metric depends on the nature of the data. Euclidean is common for continuous numerical data.\n",
    "*   **Curse of Dimensionality:** In high-dimensional spaces, the concept of \"nearest\" becomes less meaningful, as all points tend to be equidistant from each other. This can degrade KNN's performance.\n",
    "*   **Computational Cost:**\n",
    "    *   **Training:** O(1) (just storing data).\n",
    "    *   **Prediction:** O(N * D) where N is the number of training samples and D is the number of features, as it needs to calculate distance to all training points. For very large datasets, this can be slow.\n",
    "*   **Handling Imbalanced Data:** If one class is dominant, its instances might frequently be among the nearest neighbors, leading to biased predictions. Techniques like weighted voting or over/under-sampling can help.\n",
    "\n",
    "## Advantages:\n",
    "\n",
    "*   Simple to understand and implement.\n",
    "*   No training phase (lazy learner).\n",
    "*   Can be used for both classification and regression.\n",
    "*   Effective for non-linear decision boundaries.\n",
    "\n",
    "## Disadvantages:\n",
    "\n",
    "*   Computationally expensive during prediction for large datasets.\n",
    "*   Sensitive to irrelevant or redundant features.\n",
    "*   Sensitive to the scale of features.\n",
    "*   Performance degrades with high-dimensional data (curse of dimensionality).\n",
    "*   Requires sufficient memory to store the entire training dataset.\n",
    "\n",
    "## Implementation Example (Conceptual Python using scikit-learn):\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5) # K=5\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate (e.g., accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Accuracy: {accuracy:.2f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16943d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
