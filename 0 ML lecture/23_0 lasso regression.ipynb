{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d4f091",
   "metadata": {},
   "source": [
    "# **Lasso Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1317d",
   "metadata": {},
   "source": [
    "# Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that performs both regularization and variable selection to enhance prediction accuracy and interpretability.\n",
    "\n",
    "### 1. Mathematical Objective\n",
    "Lasso minimizes the sum of squared residuals plus a penalty proportional to the sum of the absolute values of the coefficients:\n",
    "\n",
    "$$ \\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\} $$\n",
    "\n",
    "Where:\n",
    "- $\\lambda \\ge 0$ is the regularization parameter (often called `alpha` in libraries like scikit-learn).\n",
    "- $\\lambda \\sum_{j=1}^{p} |\\beta_j|$ is the **L1 penalty**.\n",
    "\n",
    "### 2. Key Characteristics\n",
    "*   **Feature Selection:** Unlike Ridge (L2), Lasso can shrink coefficients to exactly zero, effectively performing automatic feature selection.\n",
    "*   **Sparsity:** It produces \"sparse\" models, which are easier to interpret in high-dimensional datasets.\n",
    "*   **Scaling Requirement:** Since the penalty is based on the magnitude of coefficients, features must be standardized (mean=0, variance=1) before fitting.\n",
    "\n",
    "### 3. Bias-Variance Trade-off\n",
    "*   **Increasing $\\lambda$:** Increases bias but decreases variance (prevents overfitting).\n",
    "*   **Decreasing $\\lambda$:** Decreases bias but increases variance (approaches OLS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83898a8",
   "metadata": {},
   "source": [
    "As you keep incresing the value of lambda in lasso regression, the model will start removing the features from the model. And it will explain the features which are most important and will only use those features, but the issue with this is it will lead to underfitting, and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f98856",
   "metadata": {},
   "source": [
    "There are 3 main things to know about alpha ($\\lambda$) in Lasso:\n",
    "\n",
    "1. Alpha = Penalty Strength: It controls how strictly you want to punish complex models.\n",
    "2. Alpha = 0 (No Penalty): This is just normal Linear Regression. You use all features, even useless ones.\n",
    "3. Alpha > 0 (Feature Killer): As you increase alpha, Lasso starts forcing the coefficients of useless features to become exactly ZERO. This effectively deletes those features from your model.\n",
    "\n",
    "- Small Alpha (e.g., 0.01): Keeps most features, slight regularization.\n",
    "- Large Alpha (e.g., 10): Keeps only the most important features, kills the rest (High Bias, Low Variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a88c4f",
   "metadata": {},
   "source": [
    "### Comparison: Ridge Regression vs. Lasso Regression\n",
    "\n",
    "| Feature | Ridge Regression ($L_2$ Regularization) | Lasso Regression ($L_1$ Regularization) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Penalty Term** | Adds squared magnitude of coefficients: $\\lambda \\sum_{j=1}^p \\beta_j^2$ | Adds absolute magnitude of coefficients: $\\lambda \\sum_{j=1}^p |\\beta_j|$ |\n",
    "| **Coefficient Shrinkage** | Shrinks coefficients asymptotically toward zero, but never exactly zero. | Can shrink coefficients exactly to zero. |\n",
    "| **Feature Selection** | Does **not** perform feature selection; retains all variables. | Performs **automatic feature selection** by nullifying irrelevant features. |\n",
    "| **Solution Type** | Analytical solution exists (Closed-form). | No closed-form solution (requires numerical optimization like Coordinate Descent). |\n",
    "| **Computational Complexity** | Generally faster to compute. | Slightly more computationally intensive due to non-differentiable absolute value. |\n",
    "| **Best Used When...** | Most features are useful and have small/medium effects. | Only a few features are significant (sparse models). |\n",
    "| **Multicollinearity** | Handles multicollinearity by distributing the penalty among correlated variables. | Tends to pick one variable from a group of correlated variables and ignores the rest. |\n",
    "\n",
    "#### Geometric Interpretation\n",
    "*   **Ridge ($L_2$):** The constraint region is a **circle/hypersphere**. The elliptical contours of the least squares error function usually hit the circle at a point where no coordinate is zero.\n",
    "*   **Lasso ($L_1$):** The constraint region is a **diamond/hyper-octahedron**. The error contours often hit the \"corners\" of the diamond on an axis, resulting in coefficients becoming exactly zero.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
