{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60f2986",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138ae3e",
   "metadata": {},
   "source": [
    "### Not using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dfa6e81",
   "metadata": {},
   "source": [
    "# Pipelines chains together multiple steps so that the output of each step is used as input to the next step.\n",
    "# Pipelines makes it easy to apply the same preprocessing to train and test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer     # handles missing data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e151fb433251ca2"
  },
  {
   "cell_type": "code",
   "id": "ce5e8ae4",
   "metadata": {},
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2433674",
   "metadata": {},
   "source": [
    "# dropping unrequired columns\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "print(df.isna().sum())\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())   # as age contains 177 NaN values, hence storing the  mean in those empty cells\n",
    "df = df.dropna(subset=['Embarked'])              # removing the NaN values from Embarked \n",
    "print()\n",
    "\n",
    "\n",
    "print(df.isna().sum())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(columns='Survived'), df['Survived'], test_size=0.2, random_state=0)\n",
    "\n",
    "# imputation\n",
    "# i have removed all the 0 values from Age and Embarked by using mean in the NaN cells, hence there is no need to use imputaion\n",
    "# still we will be using it for better understanding of the code\n",
    "si_age = SimpleImputer()\n",
    "si_embarked = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "x_train_age = si_age.fit_transform(x_train[['Age']])\n",
    "x_train_embarked = si_embarked.fit_transform(x_train[['Embarked']])\n",
    "\n",
    "x_test_age = si_age.transform(x_test[['Age']])\n",
    "x_test_embarked = si_embarked.transform(x_test[['Embarked']])\n",
    "\n",
    "\n",
    "# one hot encoding\n",
    "ohe_sex = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe_embarked = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "x_train_sex = ohe_sex.fit_transform(x_train[['Sex']])\n",
    "x_train_embarked = ohe_embarked.fit_transform(x_train[['Embarked']])\n",
    "\n",
    "x_test_sex = ohe_sex.transform(x_test[['Sex']])\n",
    "x_test_embarked = ohe_embarked.transform(x_test[['Embarked']])\n",
    "\n",
    "\n",
    "# dropping the unrequired data\n",
    "x_train_remains = x_train.drop(columns=['Sex', 'Age', 'Embarked'])\n",
    "x_test_remains = x_test.drop(columns=['Sex', 'Age', 'Embarked'])\n",
    "\n",
    "# concanating the data\n",
    "x_train_transformed = np.concatenate((x_train_remains, x_train_age, x_train_sex, x_train_embarked), axis=1)\n",
    "x_test_transformed = np.concatenate((x_test_remains, x_test_age, x_test_sex, x_test_embarked), axis=1)\n",
    "\n",
    "\n",
    "# decision tree\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train_transformed, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = clf.predict(x_test_transformed)\n",
    "print()\n",
    "print(f\"ACCURACY SCORE : {np.round(accuracy_score(y_test, y_pred), 2)*100} %\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ead709e",
   "metadata": {},
   "source": [
    "# converitng into dataframe\n",
    "\n",
    "col = (\n",
    "    list(x_train_remains.columns) +\n",
    "    ['Age'] +\n",
    "    list(ohe_sex.get_feature_names_out(['Sex'])) +\n",
    "    list(ohe_embarked.get_feature_names_out(['Embarked']))\n",
    ")\n",
    "\n",
    "x_train_transformed = pd.DataFrame(x_train_transformed, columns=col)\n",
    "x_train_transformed\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18512bd0",
   "metadata": {},
   "source": [
    "Exporting the above data to the website"
   ]
  },
  {
   "cell_type": "code",
   "id": "558ed432",
   "metadata": {},
   "source": [
    "import pickle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9f3ed9d",
   "metadata": {},
   "source": [
    "pickle.dump(ohe_sex, open('models/ohe_sex.pkl', 'wb'))\n",
    "pickle.dump(ohe_embarked, open('models/ohe_embarked.pkl', 'wb'))\n",
    "pickle.dump(clf, open('models/clf.pkl', 'wb'))\n",
    "pickle.dump(col, open('models/col.pkl', 'wb'))\n",
    "\n",
    "# all the above files will be stored in the models folder which is inside the current folder\n",
    "# go to the file \"9_1 using pickle.ipynb\", there you will learn how to use pickle "
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
