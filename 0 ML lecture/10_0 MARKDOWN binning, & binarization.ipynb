{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning & Binarization\n",
    "\n",
    "## Binning (Discretization)\n",
    "\n",
    "Binning is the process of converting **continuous numerical data into discrete bins or intervals**. It reduces the impact of minor observation errors and can reveal patterns in noisy data.\n",
    "\n",
    "### When to Use Binning:\n",
    "- When you have **continuous variables** that would benefit from being **categorical**\n",
    "- To **reduce the effect of outliers** by grouping extreme values\n",
    "- When relationships are **non-linear** and binning can capture patterns better\n",
    "- To **simplify models** by reducing the number of distinct values\n",
    "- When domain knowledge suggests **natural groupings** (e.g., age groups, income brackets)\n",
    "\n",
    "### Types of Binning:\n",
    "\n",
    "## 1. Unsupervised Binning\n",
    "\n",
    "Unsupervised binning methods **do not use the target variable** and only consider the distribution of the feature itself.\n",
    "\n",
    "### i. Equal Width Binning (Uniform Binning)\n",
    "\n",
    "- Divides the **entire range** of values into **equal-sized intervals**\n",
    "- Each bin has the **same width** (max - min) / n_bins\n",
    "- **Useful for handling outliers** by grouping them into edge bins\n",
    "- **Visual characteristic**: The histogram shape remains similar before and after binning (no changes in distribution shape)\n",
    "- **Limitation**: Can create **imbalanced bins** with very few observations in some bins if data is skewed\n",
    "\n",
    "**Example**: For ages 0-100 with 5 bins → [0-20], [20-40], [40-60], [60-80], [80-100]\n",
    "\n",
    "**Implementation**: `pd.cut()` or `KBinsDiscretizer(strategy='uniform')`\n",
    "\n",
    "---\n",
    "\n",
    "### ii. Equal Frequency Binning (Quantile Binning) ⭐ HIGHLY USED\n",
    "\n",
    "- Each bin contains approximately the **same number of observations**\n",
    "- If you want 10 bins, each interval must contain **10% of total observations**\n",
    "- The **width of intervals may NOT be equal** (varies based on data density)\n",
    "- **Useful for handling outliers** and skewed distributions\n",
    "- **Visual characteristic**: Makes the distribution **uniform** - all histogram bars will have **equal height**\n",
    "- **Most commonly used** because it ensures balanced representation across all bins\n",
    "\n",
    "**Example**: For 100 observations with 4 bins → Each bin contains exactly 25 observations, but bin widths differ\n",
    "\n",
    "**Implementation**: `pd.qcut()` or `KBinsDiscretizer(strategy='quantile')`\n",
    "\n",
    "---\n",
    "\n",
    "### iii. K-Means Binning\n",
    "\n",
    "- Uses the **K-Means clustering algorithm** to create bins\n",
    "- The intervals are based on **cluster centroids** (cluster centers)\n",
    "- Each data point is assigned to the **nearest centroid**\n",
    "- **Best used when** data naturally divides into **multiple clusters** or has multi-modal distribution\n",
    "- More sophisticated than equal-width or equal-frequency\n",
    "- If you want 5 bins → K-Means finds 5 optimal cluster centers and assigns points to nearest center\n",
    "\n",
    "**Example**: Salary data with natural clusters (entry-level, mid-level, senior, executive)\n",
    "\n",
    "**Implementation**: `KBinsDiscretizer(strategy='kmeans')`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Supervised Binning\n",
    "\n",
    "Supervised binning methods **use the target variable** to create optimal bin boundaries.\n",
    "\n",
    "### i. Decision Tree Binning\n",
    "\n",
    "- Uses a **decision tree algorithm** to determine optimal split points\n",
    "- Bins are created based on how well they **separate different target classes**\n",
    "- The algorithm finds split points that **maximize information gain** or minimize impurity\n",
    "- **Best for classification problems** where you want bins that discriminate between classes\n",
    "- Results in bins that are **most predictive** of the target variable\n",
    "- Can handle **non-linear relationships** between feature and target\n",
    "\n",
    "**Example**: Binning age based on how well different age ranges predict loan default (Yes/No)\n",
    "\n",
    "**Implementation**: Train a `DecisionTreeClassifier` with `max_leaf_nodes` parameter, then use the leaf assignments as bins\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Custom Binning\n",
    "\n",
    "- Based on **domain knowledge**, **business logic**, or **expert judgment**\n",
    "- You manually define bin boundaries that make sense for your specific use case\n",
    "- **Most interpretable** because bins have real-world meaning\n",
    "- Requires understanding of the domain and business context\n",
    "\n",
    "**Examples**:\n",
    "- Income: $0-30k → Low, $30k-70k → Middle, $70k-150k → Upper Middle, $150k+ → High\n",
    "- Age: 0-18 → Minor, 18-65 → Working Age, 65+ → Retired\n",
    "- Credit Score: <580 → Poor, 580-669 → Fair, 670-739 → Good, 740-799 → Very Good, 800+ → Exceptional\n",
    "\n",
    "**Implementation**: `pd.cut()` with custom bin edges\n",
    "\n",
    "---\n",
    "\n",
    "### Scikit-learn Tools:\n",
    "- `KBinsDiscretizer`: Flexible binning with strategies (uniform, quantile, kmeans)\n",
    "- `pd.cut()`: Pandas function for equal-width and custom binning\n",
    "- `pd.qcut()`: Pandas function for equal-frequency binning\n",
    "\n",
    "---\n",
    "\n",
    "## Binarization\n",
    "\n",
    "Binarization converts **numerical features into binary (0 or 1)** based on a threshold value.\n",
    "\n",
    "### When to Use Binarization:\n",
    "- When you only care about **whether a value exceeds a threshold**, not the actual value\n",
    "- For **presence/absence** scenarios (e.g., customer purchased: yes/no)\n",
    "- To create **indicator variables** (e.g., high risk vs low risk)\n",
    "- When **simplifying decision boundaries** improves model performance\n",
    "- For **text processing** (word present or not in a document)\n",
    "\n",
    "### How It Works:\n",
    "- Choose a **threshold value**\n",
    "- Values **≤ threshold** → 0\n",
    "- Values **> threshold** → 1\n",
    "\n",
    "### Scikit-learn Tool:\n",
    "- `Binarizer`: Converts continuous values to binary based on threshold\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "| Aspect | Binning | Binarization |\n",
    "|--------|---------|-------------|\n",
    "| **Output** | Multiple categories (3+ bins) | Binary (0 or 1) |\n",
    "| **Use Case** | Group continuous data into ranges | Simple threshold-based classification |\n",
    "| **Complexity** | More flexible, multiple bins | Simple, only 2 values |\n",
    "| **Example** | Age → Child/Teen/Adult/Senior | Temperature > 30°C → Hot (1) or Not (0) |\n",
    "\n",
    "---\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "### For Binning:\n",
    "- **Information loss**: Binning reduces granularity of data\n",
    "- **Bin selection matters**: Wrong bin boundaries can hide patterns\n",
    "- **Encoding needed**: After binning, you may need one-hot encoding for models\n",
    "- **Reversibility**: Original values cannot be recovered after binning\n",
    "\n",
    "### For Binarization:\n",
    "- **Threshold selection is critical**: Wrong threshold leads to poor results\n",
    "- **Maximum information loss**: Only preserves above/below threshold info\n",
    "- **Domain expertise needed**: Threshold should have business/scientific meaning\n",
    "- **Not always appropriate**: Use only when binary distinction is meaningful\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Examples\n",
    "\n",
    "**Binning Example:**\n",
    "- Income: \\$0-30k → Low, \\$30k-70k → Middle, \\$70k+ → High\n",
    "- Age: 0-18 → Minor, 18-65 → Adult, 65+ → Senior\n",
    "\n",
    "**Binarization Example:**\n",
    "- Blood Pressure > 140 → Hypertensive (1) or Normal (0)\n",
    "- Transaction Amount > \\$1000 → High Value (1) or Low Value (0)\n",
    "- Test Score > 75% → Pass (1) or Fail (0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
