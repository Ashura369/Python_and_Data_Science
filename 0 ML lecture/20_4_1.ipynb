{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5bdb6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8fe13a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,) \n",
      "\n",
      "COEFFICIENT :  [  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "\n",
      "INTERCEPT :  151.88331005254167\n",
      "\n",
      "\n",
      "ACCURACY :  0.4399338661568969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape, '\\n')\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print('COEFFICIENT : ', reg.coef_)\n",
    "print('\\nINTERCEPT : ', reg.intercept_)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print('\\n\\nACCURACY : ', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "927b2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SGDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            for j in range(X_train.shape[0]):\n",
    "\n",
    "                idx = np.random.randint(0, X_train.shape[0])        # will pick any one number btwn 0 and X_train[0]\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                \n",
    "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "                # b new = b old - (lr * derivative of b)\n",
    "\n",
    "                \"\"\"\n",
    "                The original formula is : (-2/m) * np.sum(y_train - y_hat)\n",
    "\n",
    "                Since we are working on stocastic gradient descent, we will remove the m and also the sum, and then the formula will becomme \n",
    "                -2 * (y_train[idx] - y_hat)\n",
    "\n",
    "                \"\"\"\n",
    "                # \"intercept_der\" (intercept derivative) is the gradient of the loss function with respect to the intercept.\n",
    "                # It represents the direction and magnitude of the error for the current sample.\n",
    "                # The -2 comes from the chain rule when differentiating the squared error (y - y_hat)^2.\n",
    "                    # d/db [(y - y_hat)^2] \n",
    "                    # = 2 * (y - y_hat) * d/db(y - (wx + b)) \n",
    "                    # = 2 * (y - y_hat) * (-1) \n",
    "                    # = -2 * (y - y_hat)\n",
    "                \n",
    "                # The error value is (y_train[idx] - y_hat), representing the raw residual of the prediction.\n",
    "                # The intercept_der is the gradient of the squared error loss function, which is the error \n",
    "                # scaled by -2 to indicate the direction and magnitude of the update needed to minimize loss.\n",
    "\n",
    "                \n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat), X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "                # m new = m old - (lr * derivative of m)\n",
    "        \n",
    "        print(self.intercept_, '\\n',self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4074f3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10)\n",
      "353\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.shape[0])\n",
    "print(X_train.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "67d75e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, X_train.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "80b6ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.82518045928134 \n",
      " [ -28.45906415 -179.29454378  555.50437855  321.72921451  -69.01448495\n",
      "  -78.15231814 -179.74588136   61.13558172  535.69130563   43.44501262]\n",
      "\n",
      "ACCURACY :  0.4445668463710014\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDRegressor(0.1, 50)     # you can even set the learning rate and the number of epocs you want\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "\n",
    "print('\\nACCURACY : ', r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e7bab",
   "metadata": {},
   "source": [
    "## **1. The Logic of the Two Loops**\n",
    "The SGD algorithm uses two nested loops to train the model.\n",
    "\n",
    "### **The Outer Loop (`epochs`)**\n",
    "*   **Code:** `for i in range(self.epochs):`\n",
    "*   **Purpose:** Controls how many times we cycle through the entire training process.\n",
    "*   **Why:** One pass through the data is rarely enough. We need to repeat the process multiple times to fine-tune the weights effectively.\n",
    "\n",
    "### **The Inner Loop (`data rows`)**\n",
    "*   **Code:** `for j in range(X_train.shape[0]):`\n",
    "*   **Purpose:** Runs once for every single row (sample) in your training data.\n",
    "*   **What happens inside:**\n",
    "    1.  Pick **ONE random row** (`idx`).\n",
    "    2.  Calculate the error for *that specific row*.\n",
    "    3.  **Immediately update** the weights (`coef_` and `intercept_`) based on that single error.\n",
    "\n",
    "\n",
    "## **2. Total Number of Updates**\n",
    "Unlike Batch Gradient Descent (which updates once per epoch), SGD updates **thousands of times per epoch**.\n",
    "\n",
    "**Formula:**\n",
    "$$ \\text{Total Updates} = \\text{Number of Rows} \\times \\text{Number of Epochs} $$\n",
    "\n",
    "**Example Scenario:**\n",
    "*   **Data Rows:** 1,000\n",
    "*   **Epochs:** 100\n",
    "\n",
    "| Loop | Iterations |\n",
    "| :--- | :--- |\n",
    "| Inner Loop | 1,000 times (per epoch) |\n",
    "| Outer Loop | 100 times |\n",
    "| **Total Weight Updates** | **$1,000 \\times 100 = 100,000$ times** |\n",
    "> *Note: It is multiplication, not power ($1000^{100}$).*\n",
    "\n",
    "\n",
    "## **3. Why We Only Keep the Final Values**\n",
    "You might wonder: *\"Doesn't the variable just get overwritten every time? Do we lose the previous learning?\"*\n",
    "**No, we don't lose learning. We build upon it.**\n",
    "*   `self.coef_` and `self.intercept_` are **persistent variables**.\n",
    "*   Every update allows the values to \"step\" closer to the optimal solution.\n",
    "**The Process:**\n",
    "1.  **Update #1:** Value starts at `1.0` (Guess) $\\rightarrow$ updates to `0.9` (Slightly better).\n",
    "2.  **Update #2:** Starts at `0.9` $\\rightarrow$ updates to `0.85` (Better).\n",
    "3.  ...\n",
    "4.  **Update #100,000:** Starts at `Optimal` $\\rightarrow$ updates to `Even More Optimal`.\n",
    "The value at the end of the 100th epoch represents the **culmination of 100,000 small improvements**. We only need this final, most accurate version to make future predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
