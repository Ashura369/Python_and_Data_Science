{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5036fa39",
   "metadata": {},
   "source": [
    "# Function Transformer & Power Transformer - Complete Guide\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Function Transformer](#function-transformer)\n",
    "3. [Power Transformer](#power-transformer)\n",
    "4. [Comparison & When to Use](#comparison--when-to-use)\n",
    "5. [Implementation Examples](#implementation-examples)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Why Transform Data?\n",
    "\n",
    "Data transformation is crucial in machine learning because:\n",
    "- **Many ML algorithms assume normally distributed data** (e.g., Linear Regression, Logistic Regression)\n",
    "- **Skewed data reduces model accuracy**\n",
    "- **Normal distribution improves convergence** in gradient-based algorithms\n",
    "- **Transformations handle outliers** more effectively\n",
    "\n",
    "### Types of Transformers\n",
    "\n",
    "1. **Function Transformers** - Apply custom mathematical functions\n",
    "2. **Power Transformers** - Box-Cox & Yeo-Johnson transformations (using log, and etc)\n",
    "3. **Quantile Transformers** - Transform to uniform or normal distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Function Transformer\n",
    "\n",
    "### What is Function Transformer?\n",
    "\n",
    "`FunctionTransformer` from sklearn allows you to apply **any custom mathematical function** to your data. It's a wrapper that makes custom transformations compatible with sklearn pipelines.\n",
    "\n",
    "### Common Mathematical Transformations\n",
    "\n",
    "#### 1. Log Transform\n",
    "```python\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Using log1p (log(1+x)) - safer with zeros\n",
    "trf = FunctionTransformer(func=np.log1p)\n",
    "```\n",
    "\n",
    "**Use Case:**\n",
    "- **Right-skewed data** (long tail on right)\n",
    "- **Positive values only**\n",
    "- Compresses large values, expands small values\n",
    "\n",
    "**Example:** Income, prices, population data\n",
    "\n",
    "#### 2. Square Root Transform\n",
    "```python\n",
    "trf = FunctionTransformer(func=np.sqrt)\n",
    "```\n",
    "\n",
    "**Use Case:**\n",
    "- **Moderately right-skewed data**\n",
    "- **Count data** (Poisson distributed)\n",
    "- Less aggressive than log transform\n",
    "\n",
    "#### 3. Square/Power Transform\n",
    "```python\n",
    "trf = FunctionTransformer(func=lambda x: x**2)\n",
    "trf = FunctionTransformer(lambda x: x ** -1)\n",
    "trf = FunctionTransformer(lambda x: np.power(x, -1))\n",
    "```\n",
    "\n",
    "**Use Case:**\n",
    "- **Left-skewed data** (long tail on left)\n",
    "- Amplifies larger values\n",
    "\n",
    "#### 4. Reciprocal Transform\n",
    "```python\n",
    "trf = FunctionTransformer(func=lambda x: 1/x)\n",
    "```\n",
    "\n",
    "**Use Case:**\n",
    "- **Highly right-skewed data**\n",
    "- Inverts magnitudes (small → large, large → small)\n",
    "\n",
    "**Warning:** Cannot use on zeros or negative values\n",
    "\n",
    "#### 5. Custom Functions\n",
    "```python\n",
    "# Custom transformation\n",
    "def custom_transform(x):\n",
    "    return np.log(x + 1) * 2\n",
    "\n",
    "trf = FunctionTransformer(func=custom_transform)\n",
    "```\n",
    "\n",
    "### Checking Data Distribution\n",
    "\n",
    "#### Method 1: Distplot (PDF - Probability Density Function)\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize distribution\n",
    "sns.histplot(data=df, x='Fare', kde=True)\n",
    "plt.show()\n",
    "```\n",
    "- Bell curve = normal distribution\n",
    "- Tail on right = right-skewed\n",
    "- Tail on left = left-skewed\n",
    "\n",
    "#### Method 2: Skewness Value\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "skewness = df['Fare'].skew()\n",
    "print(f\"Skewness: {skewness}\")\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- `0` = perfectly normal\n",
    "- `0.5 to 1` = moderately skewed\n",
    "- `> 1` = highly skewed\n",
    "- Positive = right-skewed\n",
    "- Negative = left-skewed\n",
    "\n",
    "#### Method 3: QQ Plot (Quantile-Quantile Plot)\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(df['Fare'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- Points on diagonal line = normal distribution\n",
    "- Points deviate from line = non-normal\n",
    "\n",
    "### Implementation Example\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('titanic.csv')\n",
    "x = df[['Age', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply log transform to Fare column\n",
    "trf = ColumnTransformer([\n",
    "    ('log', FunctionTransformer(np.log1p), ['Fare'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "x_train_transformed = trf.fit_transform(x_train)\n",
    "x_test_transformed = trf.transform(x_test)\n",
    "```\n",
    "\n",
    "### Real-World Example: Titanic Dataset\n",
    "\n",
    "**Problem:** Fare column is highly right-skewed\n",
    "\n",
    "**Before Transformation:**\n",
    "- Logistic Regression: 70% accuracy\n",
    "- Decision Tree: 68% accuracy\n",
    "\n",
    "**After Log Transformation:**\n",
    "- Logistic Regression: 73% accuracy ✅\n",
    "- Decision Tree: 73% accuracy ✅\n",
    "\n",
    "**Result:** +3% improvement!\n",
    "\n",
    "---\n",
    "\n",
    "## Power Transformer\n",
    "\n",
    "### What is Power Transformer?\n",
    "\n",
    "`PowerTransformer` applies **Box-Cox** or **Yeo-Johnson** transformations to make data more Gaussian-like (normally distributed). Unlike Function Transformer, it **automatically finds the optimal transformation parameter (λ lambda)**.\n",
    "\n",
    "### Import\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "```\n",
    "\n",
    "### Output Range\n",
    "Transformed values typically fall between **-3 and +3** (standardized)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Box-Cox Transformation\n",
    "\n",
    "#### Characteristics\n",
    "- **Only for positive values (x > 0)**\n",
    "- Automatically finds optimal λ (lambda) parameter\n",
    "- More powerful than simple log transform\n",
    "- Part of the power transform family\n",
    "\n",
    "#### Formula\n",
    "```\n",
    "y = (x^λ - 1) / λ   when λ ≠ 0\n",
    "y = log(x)          when λ = 0\n",
    "```\n",
    "\n",
    "#### Implementation\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Box-Cox transformation\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "x_transformed = pt.fit_transform(x_train)\n",
    "\n",
    "# View lambda parameters for each feature\n",
    "print(pt.lambdas_)\n",
    "```\n",
    "\n",
    "#### Limitations\n",
    "- **CANNOT handle zeros or negative values**\n",
    "- Data must be strictly positive (x > 0)\n",
    "- If data contains zeros or negatives → use Yeo-Johnson\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Yeo-Johnson Transformation\n",
    "\n",
    "#### Characteristics\n",
    "- **Works with ANY real numbers** (positive, negative, zero)\n",
    "- More flexible than Box-Cox\n",
    "- Default choice for most cases\n",
    "- Handles all value ranges\n",
    "\n",
    "#### Implementation\n",
    "```python\n",
    "# Yeo-Johnson transformation (handles all values)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "x_transformed = pt.fit_transform(x_train)\n",
    "\n",
    "# View lambda parameters\n",
    "print(pt.lambdas_)\n",
    "```\n",
    "\n",
    "#### Advantages over Box-Cox\n",
    "✅ Handles negative values  \n",
    "✅ Handles zeros  \n",
    "✅ More robust  \n",
    "✅ Recommended as default\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding Lambda (λ) Values\n",
    "\n",
    "The lambda parameter indicates **how much transformation** was needed:\n",
    "\n",
    "```python\n",
    "# Compare lambdas between Box-Cox and Yeo-Johnson\n",
    "pd.DataFrame({\n",
    "    'feature': x_train.columns,\n",
    "    'box_cox_lambda': pt_boxcox.lambdas_,\n",
    "    'yeo_johnson_lambda': pt_yeojohnson.lambdas_\n",
    "})\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "| Lambda (λ) | Transformation Applied |\n",
    "|------------|----------------------|\n",
    "| `λ = 1` | No transformation (x¹ = x) |\n",
    "| `λ = 0.5` | Square root (√x) |\n",
    "| `λ = 0` | Log transform |\n",
    "| `λ = -1` | Reciprocal (1/x) |\n",
    "| `λ = 2` | Square (x²) |\n",
    "\n",
    "**Important Notes:**\n",
    "- **Lower λ ≠ worse transformation**\n",
    "- Lower λ means less adjustment needed (data was already close to normal)\n",
    "- Different λ values are optimal for different distributions\n",
    "- Always validate with plots, not just λ values\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Example: Concrete Strength Dataset\n",
    "\n",
    "**Dataset:** 1030 samples, 9 features\n",
    "- Cement, Blast Furnace Slag, Fly Ash, Water, Superplasticizer, Coarse Aggregate, Fine Aggregate, Age\n",
    "- **Target:** Strength (concrete compressive strength)\n",
    "\n",
    "#### Lambda Values Comparison\n",
    "\n",
    "```\n",
    "Feature              | Box-Cox λ | Yeo-Johnson λ\n",
    "---------------------|-----------|---------------\n",
    "Cement               | 0.172     | 0.170\n",
    "Blast Furnace Slag   | 0.025     | 0.017\n",
    "Fly Ash              | -0.032    | -0.136\n",
    "Water                | 0.810     | 0.808\n",
    "Superplasticizer     | 0.100     | 0.264\n",
    "Coarse Aggregate     | 1.129     | 1.129\n",
    "Fine Aggregate       | 1.830     | 1.831\n",
    "Age                  | 0.049     | 0.002\n",
    "```\n",
    "\n",
    "**Observations:**\n",
    "- Features like Fly Ash needed more transformation (negative λ)\n",
    "- Water and aggregates needed minimal transformation (λ near 1)\n",
    "- Yeo-Johnson provides different optimal parameters\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization Function\n",
    "\n",
    "```python\n",
    "def plots(original, transformed):\n",
    "    \"\"\"Compare original vs transformed distributions\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    \n",
    "    # PDF (Probability Density Function)\n",
    "    sns.histplot(original, kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Original Distribution')\n",
    "    \n",
    "    sns.histplot(transformed, kde=True, ax=axes[1])\n",
    "    axes[1].set_title('Transformed Distribution')\n",
    "    \n",
    "    # QQ Plots\n",
    "    stats.probplot(original.flatten(), dist=\"norm\", plot=axes[2])\n",
    "    axes[2].set_title('Original QQ Plot')\n",
    "    \n",
    "    stats.probplot(transformed.flatten(), dist=\"norm\", plot=axes[3])\n",
    "    axes[3].set_title('Transformed QQ Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plots(x_train, x_train_transformed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison & When to Use\n",
    "\n",
    "### Function Transformer vs Power Transformer\n",
    "\n",
    "| Aspect | Function Transformer | Power Transformer |\n",
    "|--------|---------------------|-------------------|\n",
    "| **Transformation** | Manual (you choose function) | Automatic (finds optimal λ) |\n",
    "| **Flexibility** | Highly flexible (any function) | Fixed methods (Box-Cox/Yeo-Johnson) |\n",
    "| **Optimization** | No automatic optimization | Automatically finds best transformation |\n",
    "| **Use Case** | When you know which function to use | When unsure about transformation |\n",
    "| **Output Range** | Depends on function | Typically -3 to +3 (standardized) |\n",
    "| **Complexity** | Simple | More complex (statistical) |\n",
    "\n",
    "### Decision Tree: Which to Use?\n",
    "\n",
    "```\n",
    "Is your data skewed?\n",
    "    │\n",
    "    ├─ No → Maybe no transformation needed\n",
    "    │\n",
    "    └─ Yes → Is it right-skewed?\n",
    "            │\n",
    "            ├─ Yes → Does it contain zeros/negatives?\n",
    "            │        │\n",
    "            │        ├─ No → Try: Log Transform OR Box-Cox\n",
    "            │        │\n",
    "            │        └─ Yes → Try: log1p Transform OR Yeo-Johnson\n",
    "            │\n",
    "            └─ No (left-skewed) → Try: Square/Power Transform OR Yeo-Johnson\n",
    "```\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "#### Use Function Transformer When:\n",
    "✅ You know exactly which mathematical function to apply  \n",
    "✅ Need custom/domain-specific transformations  \n",
    "✅ Want full control over transformation  \n",
    "✅ Building pipelines with custom logic  \n",
    "\n",
    "#### Use Power Transformer When:\n",
    "✅ Unsure which transformation is best  \n",
    "✅ Want automatic optimization  \n",
    "✅ Need standardized output  \n",
    "✅ Working with multiple features simultaneously  \n",
    "✅ Want statistical rigor (Box-Cox/Yeo-Johnson are well-established)  \n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Examples\n",
    "\n",
    "### Example 1: Function Transformer in Pipeline\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create pipeline with log transformation\n",
    "pipeline = Pipeline([\n",
    "    ('log_transform', FunctionTransformer(np.log1p)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "```\n",
    "\n",
    "### Example 2: ColumnTransformer with Function Transformer\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Apply different transformations to different columns\n",
    "trf = ColumnTransformer([\n",
    "    ('log', FunctionTransformer(np.log1p), ['Fare']),\n",
    "    ('sqrt', FunctionTransformer(np.sqrt), ['Age'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "x_transformed = trf.fit_transform(x)\n",
    "```\n",
    "\n",
    "### Example 3: Power Transformer (Box-Cox)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Box-Cox (positive values only)\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "x_transformed = pt.fit_transform(x_train)\n",
    "\n",
    "# Check transformation parameters\n",
    "print(\"Lambda values:\", pt.lambdas_)\n",
    "```\n",
    "\n",
    "### Example 4: Power Transformer (Yeo-Johnson)\n",
    "\n",
    "```python\n",
    "# Yeo-Johnson (handles all values)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "x_transformed = pt.fit_transform(x_train)\n",
    "\n",
    "# In pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('power_transform', PowerTransformer(method='yeo-johnson')),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "```\n",
    "\n",
    "### Example 5: Comparing Transformations\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Original data\n",
    "lr = LogisticRegression()\n",
    "score_original = cross_val_score(lr, x_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "# With log transform\n",
    "trf = FunctionTransformer(np.log1p)\n",
    "x_train_log = trf.fit_transform(x_train[['Fare']])\n",
    "score_log = cross_val_score(lr, x_train_log, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "# With power transform\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "x_train_power = pt.fit_transform(x_train)\n",
    "score_power = cross_val_score(lr, x_train_power, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "print(f\"Original: {score_original:.4f}\")\n",
    "print(f\"Log Transform: {score_log:.4f}\")\n",
    "print(f\"Power Transform: {score_power:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Always Visualize Before & After\n",
    "```python\n",
    "# Before transformation\n",
    "sns.histplot(data=df, x='Fare', kde=True)\n",
    "plt.title('Before Transformation')\n",
    "plt.show()\n",
    "\n",
    "# Apply transformation\n",
    "trf = FunctionTransformer(np.log1p)\n",
    "df['Fare_transformed'] = trf.fit_transform(df[['Fare']])\n",
    "\n",
    "# After transformation\n",
    "sns.histplot(data=df, x='Fare_transformed', kde=True)\n",
    "plt.title('After Transformation')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2. Use Cross-Validation to Evaluate\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Don't rely on single train-test split\n",
    "# Use cross-validation for robust evaluation\n",
    "scores = cross_val_score(model, x_transformed, y, cv=5)\n",
    "print(f\"Mean accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "```\n",
    "\n",
    "### 3. Handle Zeros in Log Transform\n",
    "```python\n",
    "# ❌ Bad - will cause errors with zeros\n",
    "np.log(x)\n",
    "\n",
    "# ✅ Good - log1p handles zeros safely\n",
    "np.log1p(x)  # equivalent to log(1 + x)\n",
    "```\n",
    "\n",
    "### 4. Document Your Transformations\n",
    "```python\n",
    "# Keep track of what transformations were applied\n",
    "transformation_history = {\n",
    "    'Fare': 'log1p',\n",
    "    'Age': 'sqrt',\n",
    "    'Income': 'yeo-johnson'\n",
    "}\n",
    "```\n",
    "\n",
    "### 5. Apply Same Transformation to Test Set\n",
    "```python\n",
    "# ✅ Correct way\n",
    "trf = PowerTransformer()\n",
    "x_train_transformed = trf.fit_transform(x_train)\n",
    "x_test_transformed = trf.transform(x_test)  # Only transform, don't fit again\n",
    "\n",
    "# ❌ Wrong way - will cause data leakage\n",
    "x_test_transformed = trf.fit_transform(x_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Libraries\n",
    "\n",
    "```python\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sklearn transformers\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Function Transformer\n",
    "- Manual control over transformation\n",
    "- Use specific mathematical functions (log, sqrt, square, reciprocal)\n",
    "- Great when you know what transformation to apply\n",
    "- Flexible for custom transformations\n",
    "\n",
    "### Power Transformer\n",
    "- **Box-Cox**: Positive values only, automatic optimization\n",
    "- **Yeo-Johnson**: Works with all values (recommended default)\n",
    "- Automatically finds optimal transformation parameter (λ)\n",
    "- Standardized output (-3 to +3)\n",
    "\n",
    "### When to Transform\n",
    "- Data is significantly skewed (|skewness| > 1)\n",
    "- Model performance is suboptimal\n",
    "- Algorithm assumes normality (Linear/Logistic Regression)\n",
    "- Presence of outliers affecting model\n",
    "\n",
    "### When NOT to Transform\n",
    "- Data is already normally distributed\n",
    "- Using tree-based models (they handle non-normal data well)\n",
    "- Transformation doesn't improve validation metrics\n",
    "- Domain knowledge suggests transformation is inappropriate\n",
    "\n",
    "**Remember:** Always validate improvements using cross-validation, not just visual inspection or lambda values!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
